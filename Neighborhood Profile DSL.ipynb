{
 "metadata": {
  "name": "",
  "signature": "sha256:55fff6320efda0286482ffcbce342988ffff5018504851a6caac802d684a2526"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Dependencies"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd, csv, os\n",
      "#import numpy as np, census, us, math Not currently needed\n",
      "import requests, json\n",
      "import urllib2\n",
      "import re\n",
      "from scipy.stats import norm # needed for percent point function norm.ppf (used in significance testing)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Global Variables"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "apikey = \"827a8aafba255606ced9b44cd4380ba61ad39003\" # Drew's API key, stay cool bros"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "elmhurst = [\"06001409300\", \"06001409400\", \"06001409500\", \"06001409600\", \"06001409700\", \"06001410200\", \"06001410300\", \"06001410400\"]\n",
      "oakland = [\"0653000\"] # State 06 (CA), place 53000 (Oakland city)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Helper Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def chunks(l, n):\n",
      "    \"\"\"Returns a list of lists no longer than n (used to deal with Census API limit of 50 variables per call)\"\"\"\n",
      "    if n < 1:\n",
      "        n = 1\n",
      "    return [l[i:i + n] for i in range(0, len(l), n)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def concatenate_dataframes(geo_type, *dfs):\n",
      "    \"\"\"Stitches together multiple dataframes. Drops state, county, tract, place from dfs as they are already encoded in index\"\"\"\n",
      "    if geo_type == \"tract\":\n",
      "        masterframe = dfs[0].iloc[:,:-3]\n",
      "    elif geo_type == \"place\":\n",
      "        masterframe = dfs[0].iloc[:,:-2]\n",
      "\n",
      "    if len(dfs) > 1:\n",
      "        for df in dfs[1:]:\n",
      "            masterframe = pd.concat([masterframe, df.iloc[:,:-3 if geo_type == \"tract\" else -2]], axis = 1) \n",
      "            # axis=\"1\": concat across col instead of across rows\n",
      "            \n",
      "    return masterframe"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def se_rename(text):\n",
      "    \"\"\"Replaces \"Margin of Error\" with \"Standard Error\" in column names\"\"\"\n",
      "    try:\n",
      "        strings = re.search('(^.+ )Margin .f Error(.+$)', text)\n",
      "    except AttributeError:\n",
      "        strings = '' # apply your error handling\n",
      "\n",
      "    return strings.group(1) + \"Standard Error\" + strings.group(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Homemade Census API calling, more flexible than the census package permits\n",
      "# HUGE thanks to Aksel Olsen (via Sam Blanchard) for this function\n",
      "\n",
      "def get_data(year,dataset,table,geo_type):\n",
      "    \"\"\"\n",
      "    Gets data from api based on year, dataset and tablename\n",
      "    \"\"\"\n",
      "    \n",
      "    #concatenate table list to pass to url param\n",
      "    stable = ','.join(table)\n",
      "    \n",
      "    url = 'http://api.census.gov/data/%(yr)s/%(dset)s?key=%(key)s&get=%(tbl)s&for=%(type)s:*&in=state:06'\n",
      "    url = url %{\n",
      "    'key':apikey,\n",
      "    'yr':year,\n",
      "    'dset':dataset,\n",
      "    'tbl':stable,\n",
      "    'type': geo_type\n",
      "    }\n",
      "#    print url\n",
      "    ## get data\n",
      "    resp = requests.get(url)\n",
      "        \n",
      "    try:\n",
      "#        print year, dataset, stable, geo_type\n",
      "        data = pd.read_json(resp.text)\n",
      "        \n",
      "        ## col names are found at index 0\n",
      "        cols = data.ix[0,].tolist()\n",
      "           \n",
      "        data.columns=cols\n",
      "                        \n",
      "        ## data ships with empty row--remove, or else it won't work (DSL 10/12/2014)\n",
      "        data=data.drop(data.index[:1])\n",
      "        \n",
      "        if year == \"2000\": # Dealing with the lack of leading zeros in 2000 FIPS codes cost me well over an hour. :(\n",
      "            data[\"state\"] = data[\"state\"].apply(\"{:0>2}\".format)\n",
      "            if geo_type == \"tract\":\n",
      "                data[\"county\"] = data[\"county\"].apply(\"{:0>3}\".format)\n",
      "                data[\"tract\"] = data[\"tract\"].apply(\"{:0>6}\".format)\n",
      "            else:\n",
      "                data[\"place\"] = data[\"place\"].apply(\"{:0>5}\".format)\n",
      "\n",
      "        # Leaving in all this debris from my long struggle with this problem. FYI the correct answer is here: http://stackoverflow.com/questions/23836277/add-leading-zeros-to-strings-in-pandas-dataframe\n",
      "#        print data.head()\n",
      "#        if year == \"2000\":\n",
      "#            data.state = data.state.astype(str)\n",
      "#            total_rows['ColumnID'] = total_rows['ColumnID'].astype(str)\n",
      "#            data[\"state\"] = str(data[\"state\"]).zfill(2)\n",
      "#        print data.head()\n",
      "        ## add unique geoid as index - using ternary operator to select correct geoid format AND using zfill to ensure proper formatting of 2000 data (which normally lack leading zeros)\n",
      "        data.index = data.state + data.county + data.tract if geo_type == \"tract\" else data.state + data.place\n",
      "#        data.index = \"%02d%03d%06d\" % (data.state, data.county, data.tract) if geo_type == \"tract\" else \"%02d%05d\" % (data.state, data.place)\n",
      "#        data.index = \"%02s%03s%06s\" % (data.state, data.county, data.tract) if geo_type == \"tract\" else \"%02s%05s\" % (data.state, data.place)\n",
      "#        data.index = str(data.state).zfill(2)+str(data.county).zfill(3)+str(data.tract).zfill(6) if geo_type == \"tract\" else str(data.state).zfill(2)+str(data.place).zfill(5)\n",
      "                \n",
      "        ## make sure numeric data is numeric\n",
      "        data.ix[:,table]=data[table].astype('float64')\n",
      "        \n",
      "        return data\n",
      "    except Exception, e:\n",
      "        raise    # decoding failed\n",
      "        print \"holy smokes!\"\n",
      "        print \"failed on %s\" %url\n",
      "    else:\n",
      "        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def census_api_pull(vars_list, year, dataset, geo_type):\n",
      "    \"\"\"Takes an arbitrarily long list of variables, a year and dataset (e.g. \"sf1\", \"acs5\"), makes multiple API calls and stitches it all together\"\"\"\n",
      "    ## get variable details, labels\n",
      "    jsonurl = 'http://api.census.gov/data/%(yr)s/%(dset)s/variables.json' % {'yr': year, 'dset': dataset}\n",
      "    jsontext = requests.get(jsonurl)\n",
      "\n",
      "    ## rename columns, looking up in dictionary. Perhaps bad form w verbose names\n",
      "    datadict = pd.DataFrame(jsontext.json()['variables']).T\n",
      "        \n",
      "    ## get labels, stuff in dict\n",
      "    datadict_rename = datadict.ix[vars_list,'label'].to_dict()\n",
      "    datadict_rename_verbose = dict(zip(datadict_rename.keys(), [str(key) + \" \" + str(value) for key, value in datadict_rename.items()]))\n",
      "        \n",
      "    var_chunks = chunks(vars_list, 50) # split up the LIST into chunks of 50 variables\n",
      "    results_list = []\n",
      "    \n",
      "    for var_chunk in var_chunks:\n",
      "        df = get_data(year, dataset, var_chunk, geo_type) # perform census api pull (using Aksel's code)\n",
      "        results_list.append(df) #append the df to results_list\n",
      "        \n",
      "    masterframe = concatenate_dataframes(geo_type, *results_list) #I think the splat is needed here\n",
      "\n",
      "    ## rename: new name has both official variable name and legible description\n",
      "    masterframe.rename(columns=datadict_rename_verbose,inplace=True)\n",
      "    \n",
      "    fname = \"%s_%s_%s.csv\" % (dataset, year, geo_type)\n",
      "    print \"writing data to %s\" % fname\n",
      "    masterframe.to_csv(fname)\n",
      "    \n",
      "    return masterframe"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_dataframes(vardict):\n",
      "    \"\"\"Generates a dictionary of dataframes for various Census API pulls (year, dataset, place/tract)\"\"\"\n",
      "    dataframes = {}\n",
      "    \n",
      "    for column in vardict:\n",
      "        if column != \"Topic\" and column != \"Name\":\n",
      "            dataframes[column + \"_tract\"] = census_api_pull(vardict[column].dropna(), re.search(\"_(.+)$\", column).group(1), re.search(\"^(.+)_\", column).group(1), \"tract\")\n",
      "            dataframes[column + \"_place\"] = census_api_pull(vardict[column].dropna(), re.search(\"_(.+)$\", column).group(1), re.search(\"^(.+)_\", column).group(1), \"place\")\n",
      "    \n",
      "    return dataframes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def filter_dfs(dfdict, tract_filter, place_filter):\n",
      "    \"\"\"Returns a dict of dataframes filtered by tract and place FIPS lists\"\"\"\n",
      "    newdict = {}\n",
      "    \n",
      "    for key in dfdict:\n",
      "        if \"tract\" in key:\n",
      "            newdict[key] = dfdict[key][dfdict[key].index.isin(tract_filter)]\n",
      "        elif \"place\" in key:\n",
      "            newdict[key] = dfdict[key][dfdict[key].index.isin(place_filter)]\n",
      "    \n",
      "    return newdict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def agg_filtered_dfs(dfdict):\n",
      "    newdict = {}\n",
      "    \n",
      "    for key in dfdict:\n",
      "        newdict[key] = agg(dfdict[key])\n",
      "    \n",
      "    return newdict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def master_pull(vardict, tract_filter, place_filter):\n",
      "    dfs = get_dataframes(vardict) # Get the dataframes into a dictionary\n",
      "    print \"Retrieved data - saved to CSVs\"\n",
      "    \n",
      "    dfs_filter = filter_dfs(dfs, tract_filter, place_filter) # Filter to desired geographies\n",
      "    print \"Filtered tracts and places\"\n",
      "    \n",
      "    dfs_filter.update((x, moe_to_se(y)) for x, y in dfs_filter.items()) # Convert MOE to SE\n",
      "    print \"Converted MOE to SE\"\n",
      "    \n",
      "    dfs_filter_agg = agg_filtered_dfs(dfs_filter) # Aggregate estimates and SEs\n",
      "    print \"Aggregated estimates and SEs\"\n",
      "    \n",
      "    return dfs_filter_agg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Analysis Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def significant(x, x_se, y, y_se, ci):\n",
      "    \"\"\"Returns True if the difference between x and y, given x_se and y_se, is statistically significant at the given confidence interval\"\"\"\n",
      "    p = ci + (1 - ci)/2 # convert confidence interval into two-tailed test critical value\n",
      "    z = norm.ppf(p) # z-score\n",
      "    return abs((x - y)/(x_se**2 + y_se**2)**0.5) > z"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def moe_to_se(df):\n",
      "    \"\"\"Converts all Margin Of Error columns into Standard Error (by dividing by 1.645), and renames columns accordingly\"\"\"\n",
      "    colnames = []\n",
      "    new = df.copy()\n",
      "    \n",
      "    for colname in new.columns:\n",
      "        if \"margin of error\" in colname.lower():\n",
      "            new[colname] = new[colname] / 1.645\n",
      "            colnames.append(se_rename(colname))\n",
      "        else:\n",
      "            colnames.append(colname)\n",
      "    \n",
      "    new.columns = colnames\n",
      "    \n",
      "    return new"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def proportion_moe(num, num_moe, den, den_moe):\n",
      "    \"\"\"Calculates MOE for a proportion (use after aggregating estimates and MOEs across multiple tracts)\"\"\"\n",
      "    p = float(num) / den\n",
      "\n",
      "    if num_moe**2 - (p**2 * den_moe**2) >= 0:\n",
      "        return (num_moe**2 - (p**2 * den_moe**2))**0.5 / den\n",
      "    else:\n",
      "        return (num_moe**2 + (p**2 * den_moe**2))**0.5 / den"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def agg(df):\n",
      "    \"\"\"Aggregate census tracts for tables with counts\"\"\"\n",
      "    new = df.copy()\n",
      "    new.loc['Aggregate'] = 0\n",
      "    for i in new.columns:\n",
      "        if \"Margin Of Error\" in i or \"Standard Error\" in i:\n",
      "            new.loc[\"Aggregate\", i] = (sum(new[i]**2))**.5\n",
      "        else:\n",
      "            new.loc[\"Aggregate\", i] = sum(new[i])\n",
      "    return new.loc[\"Aggregate\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Test Code"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Just for future reference, this is basically how this will work\n",
      "\n",
      "#proportion_moe(aggregate.filter(regex = \"B01001_002E\")[0], \n",
      "#               aggregate.filter(regex = \"B01001_002M\")[0], \n",
      "#               aggregate.filter(regex = \"B01001_001E\")[0], \n",
      "#               aggregate.filter(regex = \"B01001_001M\")[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    reload(sys)\n",
      "    sys.setdefaultencoding(\"utf-8\")\n",
      "    \n",
      "# Apparently I have to do this to handle a UnicodeError that the next snippet would throw?\n",
      "# But now print statements don't output to the console...\n",
      "# Whatever, as long as I get my data correctly"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### LET'S DO THIS!!!!! ###\n",
      "\n",
      "vardict = pd.read_csv(\"vardict.csv\")\n",
      "\n",
      "results = master_pull(vardict, elmhurst, oakland)\n",
      "\n",
      "acs5_2012 = pd.concat([results[\"acs5_2012_place\"], results[\"acs5_2012_tract\"]], axis = 1)\n",
      "acs5_2012.columns = [\"Oakland\", \"Elmhurst\"]\n",
      "print \"writing data to acs5_2012.csv\"\n",
      "acs5_2012.to_csv(\"acs5_2012.csv\")\n",
      "\n",
      "sf1_2010 = pd.concat([results[\"sf1_2010_place\"], results[\"sf1_2010_tract\"]], axis = 1)\n",
      "sf1_2010.columns = [\"Oakland\", \"Elmhurst\"]\n",
      "print \"writing data to sf1_2010.csv\"\n",
      "sf1_2010.to_csv(\"sf1_2010.csv\")\n",
      "\n",
      "sf1_2000 = pd.concat([results[\"sf1_2000_place\"], results[\"sf1_2000_tract\"]], axis = 1)\n",
      "sf1_2000.columns = [\"Oakland\", \"Elmhurst\"]\n",
      "print \"writing data to sf1_2000.csv\"\n",
      "sf1_2000.to_csv(\"sf1_2000.csv\")\n",
      "\n",
      "sf3_2000 = pd.concat([results[\"sf3_2000_place\"], results[\"sf3_2000_tract\"]], axis = 1)\n",
      "sf3_2000.columns = [\"Oakland\", \"Elmhurst\"]\n",
      "print \"writing data to sf3_2000.csv\"\n",
      "sf3_2000.to_csv(\"sf3_2000.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Smaller version of LET'S DO THIS!!!!! that pulls everything but ACS5, for speed reasons\n",
      "\n",
      "vardict = pd.read_csv(\"vardict.csv\")\n",
      "\n",
      "results = master_pull(vardict.iloc[:, 3:], elmhurst, oakland)\n",
      "\n",
      "sf1_2010 = pd.concat([results[\"sf1_2010_place\"], results[\"sf1_2010_tract\"]], axis = 1)\n",
      "sf1_2010.columns = [\"Oakland\", \"Elmhurst\"]\n",
      "print \"writing data to sf1_2010.csv\"\n",
      "sf1_2010.to_csv(\"sf1_2010.csv\")\n",
      "\n",
      "sf1_2000 = pd.concat([results[\"sf1_2000_place\"], results[\"sf1_2000_tract\"]], axis = 1)\n",
      "sf1_2000.columns = [\"Oakland\", \"Elmhurst\"]\n",
      "print \"writing data to sf1_2000.csv\"\n",
      "sf1_2000.to_csv(\"sf1_2000.csv\")\n",
      "\n",
      "sf3_2000 = pd.concat([results[\"sf3_2000_place\"], results[\"sf3_2000_tract\"]], axis = 1)\n",
      "sf3_2000.columns = [\"Oakland\", \"Elmhurst\"]\n",
      "print \"writing data to sf3_2000.csv\"\n",
      "sf3_2000.to_csv(\"sf3_2000.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}